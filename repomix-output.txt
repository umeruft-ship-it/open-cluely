This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-10-01T08:46:21.574Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
.gitignore
logs.txt
package.json
src/main.js
src/preload.js
src/renderer.html
src/renderer.js
src/styles.css
src/whisper-worker.js

================================================================
Files
================================================================

================
File: .gitignore
================
# Node.js
node_modules/
dist/
build/

# Logs
logs/
*.log
npm-debug.log*

# Dependency directories
pids/
*.pid
*.seed
*.pid.lock

# Optional npm cache directory
.npm/

# Optional eslint cache
.eslintcache

# Optional REPL history
.node_repl_history

# Environment variables
.env
.env.local
.env.development.local
.env.test.local
.env.production.local

# IDE-specific files
.idea/
.vscode/
*.swp
*.bak
*.swo

# Test coverage
coverage/

# Thumbnails
.DS_Store
Thumbs.db

================
File: logs.txt
================
D:\Open-Cluely>npm start

> stealth-meeting-assistant@1.0.0 start
> electron .


tempy imported: undefined
Initializing Gemini AI...
Gemini AI initialized successfully
App is ready, creating window...
Creating stealth window...
Window position: 760, 40, size: 400x600
BrowserWindow created
Loading HTML from: D:\Open-Cluely\src\renderer.html
Applying Windows stealth settings
Content protection enabled for stealth
Window setup complete - will show after content loads
Renderer console.1: Preload script loading...
Renderer console.1: PreloadAPI: electronAPI exposed successfully
Renderer console.1: PreloadAPI: Preload script loaded successfully
Renderer console.1: DOM loaded, initializing...
Renderer console.1: Initializing renderer...
Permission check: media file:///
Renderer console.1: AudioContext created successfully
Renderer console.1: electronAPI is available
Renderer console.1: Initializing Whisper Web Worker...
Renderer console.1: Whisper worker created successfully
Renderer console.1: Renderer initialized successfully
DOM is ready
HTML finished loading
Renderer console.1: Worker message: {
  "status": "ready",
  "message": "Worker initialized and ready"
}
Renderer console.2: %cElectron Security Warning (Disabled webSecurity) font-weight: bold; This renderer process has "webSecurity" disabled. This
  exposes users of this app to severe security risks.

For more information and help, consult
https://electronjs.org/docs/tutorial/security.
This warning will not show up
once the app is packaged.
Renderer console.2: %cElectron Security Warning (allowRunningInsecureContent) font-weight: bold; This renderer process has "allowRunningInsecureContent"
  enabled. This exposes users of this app to severe security risks.


For more information and help, consult
https://electronjs.org/docs/tutorial/security.
This warning will not show up
once the app is packaged.
Renderer console.2: %cElectron Security Warning (Insecure Content-Security-Policy) font-weight: bold; This renderer process has either no Content Security
  Policy set or a policy with "unsafe-eval" enabled. This exposes users of
  this app to unnecessary security risks.

For more information and help, consult
https://electronjs.org/docs/tutorial/security.
This warning will not show up
once the app is packaged.
Renderer console.1: Content check...
Renderer console.1: Document title: AI Assistant
Renderer console.1: Body exists: true
Renderer console.1: App element exists: true
Renderer console.1: Glass container exists: true
Renderer console.1: Body made visible
Renderer console.1: App container made visible
JavaScript result: Content visibility check complete
Window shown with transparent background
Permission check: accessibility-events file:///
Renderer console.1: Setting up voice recording...
Permission check: media file:///D:/Open-Cluely/src/renderer.html
Permission check: media file:///D:/Open-Cluely/src/renderer.html
Renderer console.1: Available audio devices: 3
Renderer console.1: Trying constraint: [object Object]
Permission requested: media
Granting microphone permission
Permission check: media file:///
Permission check: media file:///D:/Open-Cluely/src/renderer.html
Renderer console.1: Microphone access granted with constraint: [object Object]
Renderer console.1: Starting recording...
Renderer console.1: Processing audio chunk...
Renderer console.1: Sending uint8Array to main process. Type: object Length: 42586
Renderer console.1: PreloadAPI: convertAudio called
Error converting audio: TypeError: Cannot read properties of undefined (reading 'file')
    at D:\Open-Cluely\src\main.js:555:29
    at WebContents.<anonymous> (node:electron/js2c/browser_init:2:77979)
    at WebContents.emit (node:events:517:28)
Renderer console.3: Error processing audio: TypeError: Cannot read properties of null (reading 'buffer')
Renderer console.1: Processing audio chunk...
Renderer console.1: Sending uint8Array to main process. Type: object Length: 48710
Renderer console.1: PreloadAPI: convertAudio called
Error converting audio: TypeError: Cannot read properties of undefined (reading 'file')
    at D:\Open-Cluely\src\main.js:555:29
    at WebContents.<anonymous> (node:electron/js2c/browser_init:2:77979)
    at WebContents.emit (node:events:517:28)
Renderer console.3: Error processing audio: TypeError: Cannot read properties of null (reading 'buffer')
Renderer console.1: Processing audio chunk...
Renderer console.1: Sending uint8Array to main process. Type: object Length: 48864
Renderer console.1: PreloadAPI: convertAudio called
Error converting audio: TypeError: Cannot read properties of undefined (reading 'file')
    at D:\Open-Cluely\src\main.js:555:29
    at WebContents.<anonymous> (node:electron/js2c/browser_init:2:77979)
    at WebContents.emit (node:events:517:28)
Renderer console.3: Error processing audio: TypeError: Cannot read properties of null (reading 'buffer')
Renderer console.1: Stopping recording...

================
File: package.json
================
{
  "name": "stealth-meeting-assistant",
  "version": "1.0.0",
  "description": "Invisible AI assistant for meetings - undetectable during screen share",
  "main": "src/main.js",
  "scripts": {
    "build-worker": "esbuild src/whisper-worker.js --bundle --platform=browser --format=esm --outfile=src/dist/whisper-worker.bundle.js --external:@xenova/transformers",
    "start": "electron .",
    "build": "electron-builder",
    "dev": "electron . --enable-logging",
    "postinstall": "electron-builder install-app-deps"
  },
  "dependencies": {
    "@google/generative-ai": "^0.2.1",
    "@xenova/transformers": "^2.17.2",
    "dotenv": "^16.3.1",
    "electron": "^28.0.0",
    "node-window-manager": "^2.2.4",
    "screenshot-desktop": "^1.15.0"
  },
  "devDependencies": {
    "electron-builder": "^24.0.0",
    "esbuild": "^0.25.6"
  },
  "build": {
    "appId": "com.stealth.meetingassistant",
    "productName": "Meeting Assistant",
    "directories": {
      "output": "dist"
    },
    "files": [
      "src/**/*",
      "package.json",
      ".env",
      "node_modules/@xenova/**/*"
    ],
    "asarUnpack": [
      "node_modules/@xenova/**/*"
    ],
    "mac": {
      "category": "public.app-category.productivity"
    },
    "win": {
      "target": "nsis"
    },
    "linux": {
      "target": "AppImage"
    }
  },
  "author": "Your Name",
  "license": "MIT"
}

================
File: src/main.js
================
(async () => {
const { app, BrowserWindow, globalShortcut, ipcMain, screen } = require('electron');

const fs = require('fs');
const os = require('os');
const path = require('path');
const screenshot = require('screenshot-desktop');
const { GoogleGenerativeAI } = require('@google/generative-ai');
require('dotenv').config();

// REMOVED: FFmpeg imports - not needed for Whisper Web approach!
// const { path: ffmpegPath } = await import('@ffmpeg-installer/ffmpeg');
// const ffmpeg = (await import('fluent-ffmpeg')).default;
// ffmpeg.setFfmpegPath(ffmpegPath);

let mainWindow;
let screenshots = [];
let chatContext = [];
const MAX_SCREENSHOTS = 3;

// Initialize Gemini AI with better error handling
let genAI = null;
let model = null;

try {
  if (!process.env.GEMINI_API_KEY) {
    console.error('GEMINI_API_KEY not found in environment variables');
  } else {
    console.log('Initializing Gemini AI...');
    genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);
    model = genAI.getGenerativeModel({ model: "gemini-2.0-flash-exp" });
    console.log('Gemini AI initialized successfully');
  }
} catch (error) {
  console.error('Failed to initialize Gemini AI:', error);
}

function createStealthWindow() {
  console.log('Creating stealth window...');
  const { width, height } = screen.getPrimaryDisplay().workAreaSize;
  
  // Adjusted window size for chat interface
  const windowWidth = 400;
  const windowHeight = 600;
  const x = Math.floor((width - windowWidth) / 2);
  const y = 40;
  
  console.log(`Window position: ${x}, ${y}, size: ${windowWidth}x${windowHeight}`);
  
  mainWindow = new BrowserWindow({
    width: windowWidth,
    height: windowHeight,
    x: x,
    y: y,
    webPreferences: {
      nodeIntegration: false,          // Disable for security
      contextIsolation: true,          // Enable for security
      preload: path.join(__dirname, 'preload.js'),
      backgroundThrottling: false,
      offscreen: false,
      webSecurity: false,              // CHANGED: Disable for microphone access
      allowRunningInsecureContent: true, // CHANGED: Allow for media access
      experimentalFeatures: false,
      enableRemoteModule: false,
      sandbox: false                   // Keep disabled for dynamic imports
    },
    frame: false,
    transparent: true,
    alwaysOnTop: true,
    skipTaskbar: true,
    resizable: false,
    minimizable: false,
    maximizable: false,
    closable: false,
    focusable: true,
    show: false,
    opacity: 1.0,
    type: 'toolbar',
    acceptFirstMouse: false,
    disableAutoHideCursor: true,
    enableLargerThanScreen: false,
    hasShadow: false,
    thickFrame: false,
    titleBarStyle: 'hidden',
    backgroundColor: '#00000000'
  });

  console.log('BrowserWindow created');
  
  const htmlPath = path.join(__dirname, 'renderer.html');
  console.log('Loading HTML from:', htmlPath);
  mainWindow.loadFile(htmlPath);
  
  // ADDED: Set up microphone permissions
  mainWindow.webContents.session.setPermissionRequestHandler((webContents, permission, callback) => {
    console.log('Permission requested:', permission);
    if (permission === 'microphone' || permission === 'media') {
      console.log('Granting microphone permission');
      callback(true);
    } else {
      console.log('Denying permission:', permission);
      callback(false);
    }
  });

  // ADDED: Set permissions policy for media access
  mainWindow.webContents.session.setPermissionCheckHandler((webContents, permission, requestingOrigin, details) => {
    console.log('Permission check:', permission, requestingOrigin);
    if (permission === 'microphone' || permission === 'media') {
      return true;
    }
    return false;
  });

  // ADDED: Override permissions for media devices
  mainWindow.webContents.session.protocol.registerFileProtocol('file', (request, callback) => {
    const pathname = decodeURI(request.url.replace('file:///', ''));
    callback(pathname);
  });
  
  // Apply stealth settings
  if (process.platform === 'darwin') {
    mainWindow.setVisibleOnAllWorkspaces(true, { 
      visibleOnFullScreen: true,
      skipTransformProcessType: true 
    });
    mainWindow.setAlwaysOnTop(true, 'pop-up-menu', 1);
    app.dock.hide();
    mainWindow.setHiddenInMissionControl(true);
  } else if (process.platform === 'win32') {
    console.log('Applying Windows stealth settings');
    mainWindow.setSkipTaskbar(true);
    mainWindow.setAlwaysOnTop(true, 'pop-up-menu');
    mainWindow.setAppDetails({
      appId: 'SystemProcess',
      appIconPath: '',
      relaunchCommand: '',
      relaunchDisplayName: ''
    });
  }
  
  mainWindow.setContentProtection(true);
  console.log('Content protection enabled for stealth');
  
  mainWindow.setIgnoreMouseEvents(false);
  
  mainWindow.webContents.on('dom-ready', () => {
    console.log('DOM is ready');
  });
  
  mainWindow.webContents.on('did-finish-load', () => {
    console.log('HTML finished loading');
    
    mainWindow.webContents.executeJavaScript(`
      console.log('Content check...');
      console.log('Document title:', document.title);
      console.log('Body exists:', !!document.body);
      console.log('App element exists:', !!document.getElementById('app'));
      console.log('Glass container exists:', !!document.querySelector('.glass-container'));
      
      document.body.style.background = 'transparent';
      
      if (document.body) {
        document.body.style.visibility = 'visible';
        document.body.style.display = 'block';
        console.log('Body made visible');
      }
      
      const app = document.getElementById('app');
      if (app) {
        app.style.visibility = 'visible';
        app.style.display = 'block';
        console.log('App container made visible');
      }
      
      'Content visibility check complete';
    `).then((result) => {
      console.log('JavaScript result:', result);
      mainWindow.show();
      mainWindow.focus();
      console.log('Window shown with transparent background');
    }).catch((error) => {
      console.log('JavaScript execution failed:', error);
      mainWindow.show();
    });
  });
  
  mainWindow.webContents.on('did-fail-load', (event, errorCode, errorDescription) => {
    console.error('Failed to load:', errorCode, errorDescription);
  });
  
  // Handle console messages from renderer
  mainWindow.webContents.on('console-message', (event, level, message, line, sourceId) => {
    console.log(`Renderer console.${level}: ${message}`);
  });
  
  if (process.env.NODE_ENV === 'development') {
    mainWindow.webContents.openDevTools();
  }
}

function registerStealthShortcuts() {
  globalShortcut.register('CommandOrControl+Alt+Shift+H', () => {
    toggleStealthMode();
  });

  globalShortcut.register('CommandOrControl+Alt+Shift+S', async () => {
    await takeStealthScreenshot();
  });

  globalShortcut.register('CommandOrControl+Alt+Shift+A', async () => {
    if (screenshots.length > 0) {
      await analyzeForMeeting();
    }
  });

  globalShortcut.register('CommandOrControl+Alt+Shift+X', () => {
    emergencyHide();
  });

  globalShortcut.register('CommandOrControl+Alt+Shift+V', () => {
    mainWindow.webContents.send('toggle-voice-recognition');
  });

  globalShortcut.register('CommandOrControl+Alt+Shift+Left', () => {
    moveToPosition('left');
  });
  
  globalShortcut.register('CommandOrControl+Alt+Shift+Right', () => {
    moveToPosition('right');
  });
  
  globalShortcut.register('CommandOrControl+Alt+Shift+Up', () => {
    moveToPosition('top');
  });
  
  globalShortcut.register('CommandOrControl+Alt+Shift+Down', () => {
    moveToPosition('bottom');
  });
}

let isVisible = true;
let autoHideTimer = null;

function toggleStealthMode() {
  if (autoHideTimer) {
    clearTimeout(autoHideTimer);
    autoHideTimer = null;
  }

  if (isVisible) {
    mainWindow.setOpacity(0.6);
    mainWindow.webContents.send('set-stealth-mode', true);
    isVisible = false;
  } else {
    mainWindow.setOpacity(1.0);
    mainWindow.webContents.send('set-stealth-mode', false);
    isVisible = true;
  }
}

function emergencyHide() {
  if (autoHideTimer) {
    clearTimeout(autoHideTimer);
    autoHideTimer = null;
  }

  mainWindow.setOpacity(0.01);
  mainWindow.webContents.send('emergency-clear');
  
  autoHideTimer = setTimeout(() => {
    if (mainWindow && !mainWindow.isDestroyed()) {
      mainWindow.setOpacity(1.0);
      isVisible = true;
    }
    autoHideTimer = null;
  }, 2000);
}

function moveToPosition(position) {
  const { width, height } = screen.getPrimaryDisplay().workAreaSize;
  const windowBounds = mainWindow.getBounds();
  
  let x, y;
  
  switch (position) {
    case 'left':
      x = 20;
      y = windowBounds.y;
      break;
    case 'right':
      x = width - windowBounds.width - 20;
      y = windowBounds.y;
      break;
    case 'top':
      x = Math.floor((width - windowBounds.width) / 2);
      y = 40;
      break;
    case 'bottom':
      x = Math.floor((width - windowBounds.width) / 2);
      y = height - windowBounds.height - 40;
      break;
    default:
      return;
  }
  
  mainWindow.setPosition(x, y);
}

async function takeStealthScreenshot() {
  try {
    console.log('Taking stealth screenshot...');
    const currentOpacity = mainWindow.getOpacity();
    
    mainWindow.setOpacity(0.01);
    
    await new Promise(resolve => setTimeout(resolve, 200));
    
    const screenshotsDir = path.join(__dirname, '..', '.stealth_screenshots');
    if (!fs.existsSync(screenshotsDir)) {
      fs.mkdirSync(screenshotsDir, { recursive: true });
    }
    
    const screenshotPath = path.join(screenshotsDir, `stealth-${Date.now()}.png`);
    await screenshot({ filename: screenshotPath });
    
    screenshots.push(screenshotPath);
    if (screenshots.length > MAX_SCREENSHOTS) {
      const oldPath = screenshots.shift();
      if (fs.existsSync(oldPath)) {
        fs.unlinkSync(oldPath);
      }
    }
    
    mainWindow.setOpacity(currentOpacity);
    
    console.log(`Screenshot saved: ${screenshotPath}`);
    console.log(`Total screenshots: ${screenshots.length}`);
    
    mainWindow.webContents.send('screenshot-taken-stealth', screenshots.length);
    
    return screenshotPath;
  } catch (error) {
    mainWindow.setOpacity(1.0);
    console.error('Stealth screenshot error:', error);
    throw error;
  }
}

async function analyzeForMeetingWithContext(context = '') {
  console.log('Starting context-aware analysis...');
  console.log('Context length:', context.length);
  console.log('API Key exists:', !!process.env.GEMINI_API_KEY);
  console.log('Model initialized:', !!model);
  console.log('Screenshots count:', screenshots.length);

  if (!process.env.GEMINI_API_KEY) {
    console.error('No GEMINI_API_KEY found');
    mainWindow.webContents.send('analysis-result', {
      error: 'No API key configured. Please add GEMINI_API_KEY to your .env file.'
    });
    return;
  }

  if (!model) {
    console.error('Gemini model not initialized');
    mainWindow.webContents.send('analysis-result', {
      error: 'AI model not initialized. Please check your API key.'
    });
    return;
  }

  if (screenshots.length === 0) {
    console.error('No screenshots to analyze');
    mainWindow.webContents.send('analysis-result', {
      error: 'No screenshots to analyze. Take a screenshot first.'
    });
    return;
  }

  try {
    console.log('Sending analysis start signal...');
    mainWindow.webContents.send('analysis-start');
    
    console.log('Processing screenshots...');
    const imageParts = await Promise.all(
      screenshots.map(async (path) => {
        console.log(`Processing screenshot: ${path}`);
        
        if (!fs.existsSync(path)) {
          console.error(`Screenshot file not found: ${path}`);
          throw new Error(`Screenshot file not found: ${path}`);
        }
        
        const imageData = fs.readFileSync(path);
        console.log(`Image data size: ${imageData.length} bytes`);
        
        return {
          inlineData: {
            data: imageData.toString('base64'),
            mimeType: 'image/png'
          }
        };
      })
    );

    console.log(`Prepared ${imageParts.length} image parts for analysis`);

    const contextPrompt = context ? `
    
CONVERSATION CONTEXT:
${context}

Based on the conversation context above and the screenshots provided, please:
1. Answer any questions that were asked in the conversation
2. Provide relevant insights about what's shown in the screenshots
3. If there are specific questions in the context, focus on answering those
4. Be concise but comprehensive

FORMAT YOUR RESPONSE AS:
    ` : '';

    const prompt = `You are an expert AI assistant for technical meetings and interviews. Analyze the provided screenshots and conversation context.

${contextPrompt}

**CODE SOLUTION:**
\`\`\`[language]
[Your complete, working code solution here - if applicable]
\`\`\`

**ANALYSIS:**
[Clear explanation of what you see in the screenshots and answers to any questions from the conversation]

**KEY INSIGHTS:**
â€¢ [Important insight 1]
â€¢ [Important insight 2]
â€¢ [Important insight 3]

Rules:
1. If there are questions in the conversation context, answer them directly
2. Provide code solutions if the screenshots show coding problems
3. Be concise but complete
4. Focus on actionable insights
5. If it's a meeting/presentation, summarize key points
6. Include time/space complexity for coding solutions

Analyze the screenshots and conversation context:`;

    console.log('Sending request to Gemini...');
    const result = await model.generateContent([prompt, ...imageParts]);
    console.log('Received response from Gemini');
    
    const response = await result.response;
    const text = response.text();
    
    console.log('Generated text length:', text.length);
    console.log('Generated text preview:', text.substring(0, 200) + '...');

    chatContext.push({
      type: 'analysis',
      content: text,
      timestamp: new Date().toISOString(),
      screenshotCount: screenshots.length
    });

    mainWindow.webContents.send('analysis-result', { text });
    console.log('Analysis result sent to renderer');
    
  } catch (error) {
    console.error('Analysis error details:', error);
    console.error('Error message:', error.message);
    console.error('Error stack:', error.stack);
    
    let errorMessage = 'Analysis failed';
    
    if (error.message.includes('API_KEY')) {
      errorMessage = 'Invalid API key. Please check your GEMINI_API_KEY.';
    } else if (error.message.includes('quota')) {
      errorMessage = 'API quota exceeded. Please try again later.';
    } else if (error.message.includes('network') || error.message.includes('fetch')) {
      errorMessage = 'Network error. Please check your internet connection.';
    } else if (error.message.includes('model')) {
      errorMessage = 'AI model error. Please try a different model.';
    } else {
      errorMessage = `Analysis failed: ${error.message}`;
    }
    
    mainWindow.webContents.send('analysis-result', {
      error: errorMessage
    });
  }
}

async function analyzeForMeeting() {
  await analyzeForMeetingWithContext();
}

// IPC handlers
ipcMain.handle('get-screenshots-count', () => {
  console.log('IPC: get-screenshots-count called, returning:', screenshots.length);
  return screenshots.length;
});

ipcMain.handle('toggle-stealth', () => {
  console.log('IPC: toggle-stealth called');
  return toggleStealthMode();
});

ipcMain.handle('emergency-hide', () => {
  console.log('IPC: emergency-hide called');
  return emergencyHide();
});

ipcMain.handle('take-stealth-screenshot', async () => {
  console.log('IPC: take-stealth-screenshot called');
  return await takeStealthScreenshot();
});

ipcMain.handle('analyze-stealth', async () => {
  console.log('IPC: analyze-stealth called');
  return await analyzeForMeeting();
});

ipcMain.handle('analyze-stealth-with-context', async (event, context) => {
  console.log('IPC: analyze-stealth-with-context called with context length:', context.length);
  return await analyzeForMeetingWithContext(context);
});

ipcMain.handle('clear-stealth', () => {
  console.log('IPC: clear-stealth called');
  screenshots.forEach(path => {
    if (fs.existsSync(path)) {
      fs.unlinkSync(path);
      console.log(`Deleted screenshot: ${path}`);
    }
  });
  screenshots = [];
  chatContext = [];
  console.log('All screenshots and context cleared');
  return { success: true };
});

ipcMain.handle('start-voice-recognition', () => {
  console.log('IPC: start-voice-recognition called');
  return { success: true };
});

ipcMain.handle('stop-voice-recognition', () => {
  console.log('IPC: stop-voice-recognition called');
  return { success: true };
});

// REMOVED: convert-audio handler - not needed with direct AudioContext approach!
// The renderer will handle audio conversion directly using AudioContext.decodeAudioData()
// This is much more reliable and simpler than FFmpeg

// App event handlers
app.whenReady().then(() => {
  console.log('App is ready, creating window...');
  createStealthWindow();
  registerStealthShortcuts();
  // Add to app.whenReady() or before createWindow
  app.commandLine.appendSwitch('enable-features', 'VaapiVideoDecoder');
  app.commandLine.appendSwitch('ignore-certificate-errors');
  app.commandLine.appendSwitch('allow-running-insecure-content');
  app.commandLine.appendSwitch('disable-web-security');
  app.commandLine.appendSwitch('enable-media-stream');
  
  isVisible = true;
  
  console.log('Window setup complete - will show after content loads');
});

app.on('window-all-closed', () => {
  // Keep running in background for stealth operation
});

app.on('activate', () => {
  if (BrowserWindow.getAllWindows().length === 0) {
    createStealthWindow();
  }
});

app.on('will-quit', () => {
  globalShortcut.unregisterAll();
  
  screenshots.forEach(path => {
    if (fs.existsSync(path)) fs.unlinkSync(path);
  });
});

app.on('web-contents-created', (event, contents) => {
  contents.on('new-window', (event, navigationUrl) => {
    event.preventDefault();
  });
  
  contents.on('will-navigate', (event, navigationUrl) => {
    if (navigationUrl !== mainWindow.webContents.getURL()) {
      event.preventDefault();
    }
  });
});

process.title = 'SystemIdleProcess';
})();

================
File: src/preload.js
================
const { contextBridge, ipcRenderer } = require('electron');

console.log('Preload script loading...');

// Expose stealth API to renderer process with enhanced error handling
try {
  contextBridge.exposeInMainWorld('electronAPI', {
    // Core stealth actions
    toggleStealth: () => {
      console.log('PreloadAPI: toggleStealth called');
      return ipcRenderer.invoke('toggle-stealth').catch(err => {
        console.error('PreloadAPI: toggleStealth error:', err);
        return { error: err.message };
      });
    },
    
    emergencyHide: () => {
      console.log('PreloadAPI: emergencyHide called');
      return ipcRenderer.invoke('emergency-hide').catch(err => {
        console.error('PreloadAPI: emergencyHide error:', err);
        return { error: err.message };
      });
    },
    
    takeStealthScreenshot: () => {
      console.log('PreloadAPI: takeStealthScreenshot called');
      return ipcRenderer.invoke('take-stealth-screenshot').catch(err => {
        console.error('PreloadAPI: takeStealthScreenshot error:', err);
        return { error: err.message };
      });
    },
    
    analyzeStealth: () => {
      console.log('PreloadAPI: analyzeStealth called');
      return ipcRenderer.invoke('analyze-stealth').catch(err => {
        console.error('PreloadAPI: analyzeStealth error:', err);
        return { error: err.message };
      });
    },
    
    analyzeStealthWithContext: (context) => {
      console.log('PreloadAPI: analyzeStealthWithContext called with context length:', context?.length || 0);
      return ipcRenderer.invoke('analyze-stealth-with-context', context).catch(err => {
        console.error('PreloadAPI: analyzeStealthWithContext error:', err);
        return { error: err.message };
      });
    },
    
    clearStealth: () => {
      console.log('PreloadAPI: clearStealth called');
      return ipcRenderer.invoke('clear-stealth').catch(err => {
        console.error('PreloadAPI: clearStealth error:', err);
        return { error: err.message };
      });
    },
    
    getScreenshotsCount: () => {
      console.log('PreloadAPI: getScreenshotsCount called');
      return ipcRenderer.invoke('get-screenshots-count').catch(err => {
        console.error('PreloadAPI: getScreenshotsCount error:', err);
        return 0;
      });
    },
    
    // Voice functionality
    startVoiceRecognition: () => {
      console.log('PreloadAPI: startVoiceRecognition called');
      return ipcRenderer.invoke('start-voice-recognition').catch(err => {
        console.error('PreloadAPI: startVoiceRecognition error:', err);
        return { error: err.message };
      });
    },
    
    stopVoiceRecognition: () => {
      console.log('PreloadAPI: stopVoiceRecognition called');
      return ipcRenderer.invoke('stop-voice-recognition').catch(err => {
        console.error('PreloadAPI: stopVoiceRecognition error:', err);
        return { error: err.message };
      });
    },

    convertAudio: (audioData) => {
      console.log('PreloadAPI: convertAudio called');
      return ipcRenderer.invoke('convert-audio', audioData).catch(err => {
        console.error('PreloadAPI: convertAudio error:', err);
        return null;
      });
    },
    
    // Event listeners with cleanup functions and error handling
    onScreenshotTakenStealth: (callback) => {
      const handler = (event, count) => {
        console.log('PreloadAPI: onScreenshotTakenStealth event received, count:', count);
        try {
          callback(count);
        } catch (err) {
          console.error('PreloadAPI: onScreenshotTakenStealth callback error:', err);
        }
      };
      ipcRenderer.on('screenshot-taken-stealth', handler);
      return () => {
        console.log('PreloadAPI: removing onScreenshotTakenStealth listener');
        ipcRenderer.removeListener('screenshot-taken-stealth', handler);
      };
    },
    
    onAnalysisStart: (callback) => {
      const handler = () => {
        console.log('PreloadAPI: onAnalysisStart event received');
        try {
          callback();
        } catch (err) {
          console.error('PreloadAPI: onAnalysisStart callback error:', err);
        }
      };
      ipcRenderer.on('analysis-start', handler);
      return () => {
        console.log('PreloadAPI: removing onAnalysisStart listener');
        ipcRenderer.removeListener('analysis-start', handler);
      };
    },
    
    onAnalysisResult: (callback) => {
      const handler = (event, data) => {
        console.log('PreloadAPI: onAnalysisResult event received, data type:', typeof data);
        try {
          callback(data);
        } catch (err) {
          console.error('PreloadAPI: onAnalysisResult callback error:', err);
        }
      };
      ipcRenderer.on('analysis-result', handler);
      return () => {
        console.log('PreloadAPI: removing onAnalysisResult listener');
        ipcRenderer.removeListener('analysis-result', handler);
      };
    },
    
    onSetStealthMode: (callback) => {
      const handler = (event, enabled) => {
        console.log('PreloadAPI: onSetStealthMode event received, enabled:', enabled);
        try {
          callback(enabled);
        } catch (err) {
          console.error('PreloadAPI: onSetStealthMode callback error:', err);
        }
      };
      ipcRenderer.on('set-stealth-mode', handler);
      return () => {
        console.log('PreloadAPI: removing onSetStealthMode listener');
        ipcRenderer.removeListener('set-stealth-mode', handler);
      };
    },
    
    onEmergencyClear: (callback) => {
      const handler = () => {
        console.log('PreloadAPI: onEmergencyClear event received');
        try {
          callback();
        } catch (err) {
          console.error('PreloadAPI: onEmergencyClear callback error:', err);
        }
      };
      ipcRenderer.on('emergency-clear', handler);
      return () => {
        console.log('PreloadAPI: removing onEmergencyClear listener');
        ipcRenderer.removeListener('emergency-clear', handler);
      };
    },
    
    onError: (callback) => {
      const handler = (event, message) => {
        console.log('PreloadAPI: onError event received, message:', message);
        try {
          callback(message);
        } catch (err) {
          console.error('PreloadAPI: onError callback error:', err);
        }
      };
      ipcRenderer.on('error', handler);
      return () => {
        console.log('PreloadAPI: removing onError listener');
        ipcRenderer.removeListener('error', handler);
      };
    },
    
    // Voice recognition events
    onVoiceTranscript: (callback) => {
      const handler = (event, data) => {
        console.log('PreloadAPI: onVoiceTranscript event received');
        try {
          callback(data);
        } catch (err) {
          console.error('PreloadAPI: onVoiceTranscript callback error:', err);
        }
      };
      ipcRenderer.on('voice-transcript', handler);
      return () => {
        console.log('PreloadAPI: removing onVoiceTranscript listener');
        ipcRenderer.removeListener('voice-transcript', handler);
      };
    },
    
    onVoiceError: (callback) => {
      const handler = (event, error) => {
        console.log('PreloadAPI: onVoiceError event received, error:', error);
        try {
          callback(error);
        } catch (err) {
          console.error('PreloadAPI: onVoiceError callback error:', err);
        }
      };
      ipcRenderer.on('voice-error', handler);
      return () => {
        console.log('PreloadAPI: removing onVoiceError listener');
        ipcRenderer.removeListener('voice-error', handler);
      };
    },
    
    // Utility functions for debugging
    log: (message) => {
      console.log('PreloadAPI log:', message);
    },
    
    // Check if electronAPI is working
    isAvailable: () => {
      console.log('PreloadAPI: isAvailable check');
      return true;
    }
  });

  console.log('PreloadAPI: electronAPI exposed successfully');

} catch (error) {
  console.error('PreloadAPI: Failed to expose electronAPI:', error);
}

// Global error handler for preload script
process.on('uncaughtException', (error) => {
  console.error('PreloadAPI: Uncaught exception:', error);
});

process.on('unhandledRejection', (reason, promise) => {
  console.error('PreloadAPI: Unhandled rejection at:', promise, 'reason:', reason);
});

console.log('PreloadAPI: Preload script loaded successfully');

================
File: src/renderer.html
================
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AI Assistant</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div id="app" class="glass-container">
    <!-- Compact Interface - Horizontal Layout like Cluely -->
    <div class="main-interface">
      <!-- Status Section with Timer and Count -->
      <div class="status-section">
        <div class="status-dot"></div>
        <span class="timer">00:00</span>
        <div class="screenshot-count" id="screenshot-count">0</div>
      </div>
      
      <!-- Action Buttons - Compact -->
      <div class="content-section">
        <div class="action-buttons">
          <button id="voice-toggle" class="action-btn voice-btn" title="Toggle Voice Recognition (Ctrl+Alt+Shift+V)">
            <svg width="12" height="12" viewBox="0 0 24 24" fill="currentColor">
              <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"/>
              <path d="M19 10v2a7 7 0 0 1-14 0v-2"/>
              <path d="M12 19v4"/>
              <path d="M8 23h8"/>
            </svg>
          </button>
          
          <button id="screenshot-btn" class="action-btn screenshot-btn" title="Screenshot (Ctrl+Alt+Shift+S)">
            <svg width="12" height="12" viewBox="0 0 24 24" fill="currentColor">
              <path d="M9 2 7.17 4H4c-1.1 0-2 .9-2 2v12c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2h-3.17L15 2H9zm3 15c-2.76 0-5-2.24-5-5s2.24-5 5-5 5 2.24 5 5-2.24 5-5 5z"/>
            </svg>
          </button>
          
          <button id="analyze-btn" class="action-btn analyze-btn" title="Ask AI (Ctrl+Alt+Shift+A)">
            Ask AI
          </button>
          
          <button id="clear-btn" class="action-btn clear-btn" title="Clear">
            <svg width="12" height="12" viewBox="0 0 24 24" fill="currentColor">
              <path d="M6 19c0 1.1.9 2 2 2h8c1.1 0 2-.9 2-2V7H6v12zM19 4h-3.5l-1-1h-5l-1 1H5v2h14V4z"/>
            </svg>
          </button>
          
          <button id="hide-btn" class="action-btn hide-btn" title="Emergency Hide (Ctrl+Alt+Shift+X)">
            <svg width="12" height="12" viewBox="0 0 24 24" fill="currentColor">
              <path d="M18.3 5.71a1 1 0 0 0-1.41 0L12 10.59 7.11 5.7a1 1 0 1 0-1.42 1.42L10.59 12l-4.9 4.89a1 1 0 0 0 1.42 1.42L12 13.41l4.89 4.9a1 1 0 0 0 1.42-1.42L13.41 12l4.9-4.89a1 1 0 0 0 0-1.4Z"/>
            </svg>
          </button>
        </div>
      </div>
    </div>
    
    <!-- Chat Interface -->
    <div id="chat-container" class="chat-container">
      <div class="chat-header">
        <span class="chat-title">ðŸ¤– AI Assistant</span>
        <div class="chat-controls">
          <button id="copy-btn" class="copy-btn" title="Copy Last Response">
            <svg width="12" height="12" viewBox="0 0 24 24" fill="currentColor">
              <path d="M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm3 4H8c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h11c1.1 0 2-.9 2-2V7c0-1.1-.9-2-2-2zm0 16H8V7h11v14z"/>
            </svg>
          </button>
        </div>
      </div>
      <div id="chat-messages" class="chat-messages">
        <!-- Messages will be added here dynamically -->
      </div>
    </div>
    
    <!-- Hidden status text for feedback -->
    <div id="status-text" class="status-text" style="display: none;">AI Assistant</div>
    
    <!-- Expandable Results Panel (Legacy - now using chat) -->
    <div id="results-panel" class="results-panel hidden">
      <div class="results-header">
        <div class="results-title">
          <span class="ai-response-icon">âœ¨</span>
          <span>AI Response</span>
        </div>
        <div class="results-actions">
          <button id="close-results" class="close-btn" title="Close">
            <svg width="14" height="14" viewBox="0 0 24 24" fill="currentColor">
              <path d="M18.3 5.71a1 1 0 0 0-1.41 0L12 10.59 7.11 5.7a1 1 0 1 0-1.42 1.42L10.59 12l-4.9 4.89a1 1 0 0 0 1.42 1.42L12 13.41l4.89 4.9a1 1 0 0 0 1.42-1.42L13.41 12l4.9-4.89a1 1 0 0 0 0-1.4Z"/>
            </svg>
          </button>
        </div>
      </div>
      <div class="results-content">
        <div id="result-text" class="result-text"></div>
      </div>
    </div>
    
    <!-- Loading Overlay -->
    <div id="loading-overlay" class="loading-overlay hidden">
      <div class="loading-content">
        <div class="analyzing-icon">ðŸ§ </div>
        <div class="loading-text">Analyzing screen...</div>
      </div>
    </div>
    
    <!-- Emergency Overlay -->
    <div id="emergency-overlay" class="emergency-overlay hidden">
      <div class="emergency-message">HIDDEN</div>
    </div>
  </div>

  <script src="renderer.js"></script>
</body>
</html>

================
File: src/renderer.js
================
// Fixed renderer.js with proper Whisper Worker implementation

let screenshotsCount = 0;
let isAnalyzing = false;
let stealthModeActive = false;
let stealthHideTimeout = null;
let isRecording = false;
let mediaRecorder = null;
let audioStream = null;
let recordingChunks = [];
let chatMessagesArray = [];
let isModelLoading = false;
let audioContext = null;

// Whisper worker
let worker = null;
let isWorkerReady = false;

// DOM elements
const statusText = document.getElementById('status-text');
const screenshotCount = document.getElementById('screenshot-count');
const resultsPanel = document.getElementById('results-panel');
const resultText = document.getElementById('result-text');
const loadingOverlay = document.getElementById('loading-overlay');
const emergencyOverlay = document.getElementById('emergency-overlay');
const chatContainer = document.getElementById('chat-container');
const chatMessagesElement = document.getElementById('chat-messages');
const voiceToggle = document.getElementById('voice-toggle');

const screenshotBtn = document.getElementById('screenshot-btn');
const analyzeBtn = document.getElementById('analyze-btn');
const clearBtn = document.getElementById('clear-btn');
const hideBtn = document.getElementById('hide-btn');
const copyBtn = document.getElementById('copy-btn');
const closeResultsBtn = document.getElementById('close-results');

// Timer
let startTime = Date.now();
let timerInterval;

// Whisper configuration
const WHISPER_CONFIG = {
    model: 'Xenova/whisper-tiny.en', // English-only for better performance
    multilingual: false,
    quantized: true,
    subtask: 'transcribe',
    language: 'english'
};


// Initialize
async function init() {
    console.log('Initializing renderer...');
    
    // Create a single AudioContext
    try {
        audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 16000
        });
        console.log('AudioContext created successfully');
    } catch (error) {
        console.error('Failed to create AudioContext:', error);
        showFeedback('Audio processing disabled', 'error');
    }
    
    // Check if electronAPI is available
    if (typeof window.electronAPI !== 'undefined') {
        console.log('electronAPI is available');
    } else {
        console.error('electronAPI not available');
        showFeedback('electronAPI not available', 'error');
    }
    
    await initializeWhisperWorker();
    setupEventListeners();
    setupIpcListeners();
    updateUI();
    startTimer();
    stealthModeActive = false;
    
    document.body.style.visibility = 'visible';
    document.body.style.display = 'block';
    const app = document.getElementById('app');
    if (app) {
        app.style.visibility = 'visible';
        app.style.display = 'block';
    }
    
    console.log('Renderer initialized successfully');
}

async function initializeWhisperWorker() {
    try {
        console.log('Initializing Whisper Web Worker...');
        
        // Since you have the bundled version in dist/, use that
        // Use relative path from the renderer.html location
        worker = new Worker('./dist/whisper-worker.bundle.js', { type: 'module' });
        
        worker.addEventListener('message', handleWorkerMessage);
        worker.addEventListener('error', (error) => {
            console.error('Worker error:', error);
            showFeedback('Speech recognition worker failed', 'error');
        });
        
        console.log('Whisper worker created successfully');
        return true;
        
    } catch (error) {
        console.error('Failed to initialize Whisper worker:', error);
        showFeedback('Failed to initialize speech recognition', 'error');
        
        // Fallback: try the unbundled version
        try {
            console.log('Trying fallback worker path...');
            worker = new Worker('./whisper-worker.js', { type: 'module' });
            
            worker.addEventListener('message', handleWorkerMessage);
            worker.addEventListener('error', (error) => {
                console.error('Fallback worker error:', error);
                showFeedback('Speech recognition worker failed', 'error');
            });
            
            console.log('Fallback whisper worker created successfully');
            return true;
            
        } catch (fallbackError) {
            console.error('Fallback worker also failed:', fallbackError);
            return false;
        }
    }
}   

// Handle worker messages
function handleWorkerMessage(event) {
    const message = event.data;
    
    console.log('Worker message:', JSON.stringify(message, null, 2));
    
    switch (message.status) {
        case "progress":
            const percent = Math.round((message.progress || 0) * 100);
            showFeedback(`Loading model: ${percent}%`, 'info');
            break;
            
        case "update":
            // Handle interim results but don't add to chat yet
            if (message.data && message.data[0]) {
                const interimText = message.data[0].trim();
                console.log('Interim transcription:', interimText);
                
                // Only show meaningful interim results
                if (interimText.length > 3 && !isNoise(interimText)) {
                    // You could display this as interim feedback
                    // showFeedback(`Hearing: "${interimText}"`, 'info');
                }
            }
            break;
            
        case "complete":
            if (message.data && message.data[0]) {
                const transcribedText = message.data[0].trim();
                console.log('Final transcription result:', transcribedText);
                
                // More strict filtering for final results
                if (transcribedText.length > 3 && !isNoise(transcribedText) && !isRepeatedPattern(transcribedText)) {
                    addChatMessage('voice', transcribedText);
                    showFeedback('Voice captured', 'success');
                } else {
                    console.log('Filtered out noise/invalid transcription:', transcribedText);
                }
            }
            break;
            
        case "initiate":
            isModelLoading = true;
            showFeedback('Loading speech recognition model...', 'info');
            break;
            
        case "ready":
            isModelLoading = false;
            isWorkerReady = true;
            showFeedback('Speech recognition ready', 'success');
            break;
            
        case "error":
            console.error('Worker error:', message.data);
            showFeedback('Speech recognition error', 'error');
            isModelLoading = false;
            break;
    }
    
    updateUI();
}


// Setup audio recording
// Alternative microphone access approach
async function setupVoiceRecording() {
  try {
    console.log('Setting up voice recording...');
    
    // Check if getUserMedia is available
    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
      throw new Error('getUserMedia not supported');
    }
    
    // Request permissions first
    const devices = await navigator.mediaDevices.enumerateDevices();
    const audioDevices = devices.filter(device => device.kind === 'audioinput');
    console.log('Available audio devices:', audioDevices.length);
    
    if (audioDevices.length === 0) {
      throw new Error('No microphone devices found');
    }
    
    // Try different constraint configurations
    const constraints = [
      // Simple constraint
      { audio: true },
      // Detailed constraint
      {
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
          channelCount: 1
        }
      },
      // Minimal constraint
      {
        audio: {
          sampleRate: 16000
        }
      }
    ];
    
    let stream = null;
    let lastError = null;
    
    for (const constraint of constraints) {
      try {
        console.log('Trying constraint:', constraint);
        stream = await navigator.mediaDevices.getUserMedia(constraint);
        console.log('Microphone access granted with constraint:', constraint);
        break;
      } catch (error) {
        console.warn('Failed with constraint:', constraint, error);
        lastError = error;
      }
    }
    
    if (!stream) {
      throw lastError || new Error('All microphone access attempts failed');
    }
    
    audioStream = stream;
    return true;
    
} catch (error) {
  console.error('Microphone access error details:');
  console.error('Error name:', error.name);
  console.error('Error message:', error.message);
  console.error('Error constraint:', error.constraint);
  console.error('Full error:', error);
  
  let userMessage = 'Microphone access denied';
  
  switch (error.name) {
    case 'NotAllowedError':
      userMessage = 'Microphone permission denied. Check browser/system settings.';
      break;
    case 'NotFoundError':
      userMessage = 'No microphone found. Please connect a microphone.';
      break;
    case 'NotReadableError':
      userMessage = 'Microphone is being used by another application.';
      break;
    case 'OverconstrainedError':
      userMessage = 'Microphone constraints not supported. Trying simpler settings...';
      break;
    case 'SecurityError':
      userMessage = 'Microphone access blocked due to security policy.';
      break;
    default:
      userMessage = `Microphone error: ${error.message}`;
  }
  
  showFeedback(userMessage, 'error');
  return false;
}
}

// Start recording
async function startAutoRecording() {
    if (isRecording || !isWorkerReady) {
        console.log('Cannot start recording:', { isRecording, isWorkerReady });
        return;
    }
    
    if (!audioStream) {
        const success = await setupVoiceRecording();
        if (!success) return;
    }
    
    try {
        isRecording = true;
        updateVoiceUI();
        
        console.log('Starting recording...');
        startChunkedRecording();
        
        addChatMessage('system', 'Voice recording started...');
        showFeedback('Recording started', 'success');
        
    } catch (error) {
        console.error('Failed to start recording:', error);
        showFeedback('Recording failed', 'error');
        isRecording = false;
        updateVoiceUI();
    }
}

// Stop recording
function stopAutoRecording() {
    if (!isRecording) return;
    
    console.log('Stopping recording...');
    
    isRecording = false;
    
    if (mediaRecorder && mediaRecorder.state !== 'inactive') {
        mediaRecorder.stop();
    }
    
    updateVoiceUI();
    addChatMessage('system', 'Voice recording stopped');
    showFeedback('Recording stopped', 'info');
}

// Start chunked recording
function startChunkedRecording() {
    const chunkDuration = 3000; // 3 seconds for better audio quality
    const silenceTimeout = 100;  // Minimal gap between chunks
    
    function recordChunk() {
        if (!isRecording) return;
        
        recordingChunks = [];
        
        try {
            // Better MediaRecorder configuration
            const options = {
                mimeType: 'audio/webm;codecs=opus',
                bitsPerSecond: 32000 // Higher bitrate for better quality
            };
            
            // Fallback for different browsers
            if (!MediaRecorder.isTypeSupported(options.mimeType)) {
                options.mimeType = 'audio/webm';
            }
            
            mediaRecorder = new MediaRecorder(audioStream, options);
            
            mediaRecorder.ondataavailable = (event) => {
                if (event.data.size > 500) { // Only process substantial chunks
                    recordingChunks.push(event.data);
                    console.log('Audio chunk received, size:', event.data.size);
                }
            };
            
            mediaRecorder.onstop = async () => {
                console.log('MediaRecorder stopped, chunks:', recordingChunks.length);
                
                if (recordingChunks.length > 0 && isRecording) {
                    await processAudioChunk();
                }
                
                if (isRecording) {
                    setTimeout(recordChunk, silenceTimeout);
                }
            };
            
            mediaRecorder.onerror = (event) => {
                console.error('MediaRecorder error:', event.error);
                if (isRecording) {
                    setTimeout(recordChunk, 1000); // Retry after error
                }
            };
            
            mediaRecorder.start();
            console.log('MediaRecorder started');
            
            setTimeout(() => {
                if (mediaRecorder && mediaRecorder.state === 'recording') {
                    console.log('Stopping MediaRecorder after', chunkDuration, 'ms');
                    mediaRecorder.stop();
                }
            }, chunkDuration);
            
        } catch (error) {
            console.error('Error in recordChunk:', error);
            if (isRecording) {
                setTimeout(recordChunk, 1000); // Retry after 1 second
            }
        }
    }
    
    recordChunk();
}


// Process audio chunk
async function processAudioChunk() {
    if (!worker || !isWorkerReady || recordingChunks.length === 0) return;
    
    try {
        console.log('Processing audio chunk...');
        
        const audioBlob = new Blob(recordingChunks, { type: 'audio/webm' });
        
        // Check if audio is substantial enough to process
        if (audioBlob.size < 2000) { // Less than 2KB, likely silence
            console.log('Audio chunk too small, skipping. Size:', audioBlob.size);
            return;
        }
        
        const audioBuffer = await convertBlobToAudioBuffer(audioBlob);
        
        if (!audioBuffer || audioBuffer.length === 0) {
            console.warn('No audio data to process');
            return;
        }
        
        // Enhanced audio quality checks
        const maxAmplitude = Math.max(...audioBuffer.map(Math.abs));
        const rms = Math.sqrt(audioBuffer.reduce((sum, val) => sum + val * val, 0) / audioBuffer.length);
        
        console.log(`Audio quality - Length: ${audioBuffer.length}, Max: ${maxAmplitude.toFixed(4)}, RMS: ${rms.toFixed(4)}`);
        
        // More strict thresholds
        const minAmplitude = 0.005; // Minimum peak amplitude
        const minRMS = 0.001;       // Minimum RMS energy
        
        if (maxAmplitude < minAmplitude || rms < minRMS) {
            console.log(`Audio too quiet - Max: ${maxAmplitude.toFixed(4)} < ${minAmplitude}, RMS: ${rms.toFixed(4)} < ${minRMS}`);
            return;
        }
        
        // Check for minimum duration (at least 0.5 seconds at 16kHz)
        const minSamples = 8000; // 0.5 seconds at 16kHz
        if (audioBuffer.length < minSamples) {
            console.log(`Audio too short: ${audioBuffer.length} samples < ${minSamples} required`);
            return;
        }
        
        console.log('Sending quality audio to Whisper worker');
        
        // Send to worker
        worker.postMessage({
            audio: audioBuffer,
            model: WHISPER_CONFIG.model,
            multilingual: WHISPER_CONFIG.multilingual,
            quantized: WHISPER_CONFIG.quantized,
            subtask: WHISPER_CONFIG.subtask,
            language: WHISPER_CONFIG.language
        });
        
    } catch (error) {
        console.error('Error processing audio:', error);
    }
}

// Convert audio blob to format for Whisper
async function convertBlobToAudioBuffer(blob) {
    try {
        const arrayBuffer = await blob.arrayBuffer();
        const uint8Array = new Uint8Array(arrayBuffer);
        
        console.log('Converting blob to audio buffer. Size:', uint8Array.length);
        
        if (uint8Array.length === 0) {
            console.warn('Empty audio data');
            return null;
        }
        
        // Convert using main process
        const audioBuffer = await window.electronAPI.convertAudio(uint8Array);
        
        if (!audioBuffer) {
            console.error('Audio conversion failed');
            return null;
        }
        
        console.log('Received audio buffer from main process, size:', audioBuffer.length);
        
        // Convert Buffer/ArrayBuffer to Float32Array for Whisper
        let rawBuffer;
        if (audioBuffer.buffer) {
            rawBuffer = audioBuffer.buffer;
        } else {
            rawBuffer = audioBuffer;
        }
        
        // Read as 16-bit PCM and convert to Float32Array
        const dataView = new DataView(rawBuffer);
        const numSamples = Math.floor(rawBuffer.byteLength / 2); // 16-bit = 2 bytes per sample
        const float32Array = new Float32Array(numSamples);
        
        for (let i = 0; i < numSamples; i++) {
            // Read 16-bit signed integer (little endian)
            const int16 = dataView.getInt16(i * 2, true);
            // Convert to float [-1, 1] range
            float32Array[i] = int16 / 32768.0;
        }
        
        console.log('Audio converted to Float32Array:', float32Array.length, 'samples');
        
        // Calculate some audio statistics
        const maxAmplitude = Math.max(...float32Array.map(Math.abs));
        const rms = Math.sqrt(float32Array.reduce((sum, val) => sum + val * val, 0) / float32Array.length);
        
        console.log('Audio stats - Max amplitude:', maxAmplitude.toFixed(4), 'RMS:', rms.toFixed(4));
        
        return float32Array;
        
    } catch (error) {
        console.error('Error converting blob to audio buffer:', error);
        return null;
    }
}

// Filter noise
function isNoise(text) {
    const cleanText = text.trim().toLowerCase();
    
    // Filter out noise patterns
    const noisePatterns = [
        /^[^\w\s]*$/,                    // Only punctuation
        /^[!@#$%^&*()_+\-=\[\]{}|;':".,<>?\/`~]*$/, // Only symbols
        /^(uh|um|er|ah|oh|hmm)+$/i,      // Filler words
        /^[.!?]+$/,                      // Only punctuation
        /^silence$/i,                    // Literal "silence"
        /^background/i,                  // Background noise
        /^[!]{2,}$/,                     // Multiple exclamation marks (your issue!)
        /^[\s]*$/,                       // Only whitespace
        /^[a-z]$/i,                      // Single characters
        /^[0-9]+$/,                      // Only numbers
        /^[.,;:!?]+$/,                   // Only punctuation
        /^music$/i,                      // Music detection
        /^noise$/i,                      // Noise detection
    ];
    
    // Check length - very short transcriptions are usually noise
    if (cleanText.length < 3) {
        return true;
    }
    
    // Check against patterns
    return noisePatterns.some(pattern => pattern.test(cleanText));
}

function isRepeatedPattern(text) {
    // Check for repeated single characters
    if (/^(.)\1{3,}$/.test(text)) {
        return true;
    }
    
    // Check for repeated short patterns
    if (text.length > 6) {
        for (let len = 1; len <= 3; len++) {
            const pattern = text.substring(0, len);
            const repeated = pattern.repeat(Math.floor(text.length / len));
            if (text.startsWith(repeated) && repeated.length >= text.length * 0.8) {
                return true;
            }
        }
    }
    
    return false;
}

// Toggle voice recognition
async function toggleVoiceRecognition() {
    if (isRecording) {
        stopAutoRecording();
        voiceToggle.classList.remove('active');
    } else {
        await startAutoRecording();
        if (isRecording) {
            voiceToggle.classList.add('active');
        }
    }
}

// Update voice UI
function updateVoiceUI() {
    if (!voiceToggle) return;
    
    if (isRecording) {
        voiceToggle.classList.add('listening');
        voiceToggle.title = 'Stop recording';
    } else {
        voiceToggle.classList.remove('listening');
        
        if (!isWorkerReady) {
            voiceToggle.title = 'Loading speech recognition...';
            voiceToggle.disabled = true;
        } else {
            voiceToggle.title = 'Start recording';
            voiceToggle.disabled = false;
        }
    }
}

// Add chat message
function addChatMessage(type, content) {
    if (!chatMessagesElement) return;
    
    const messageDiv = document.createElement('div');
    messageDiv.className = `chat-message ${type}`;
    
    const timestamp = new Date().toLocaleTimeString([], { 
        hour: '2-digit', 
        minute: '2-digit' 
    });
    
    let messageContent = '';
    
    switch (type) {
        case 'voice':
            messageContent = `<div class="message-header"><span class="message-icon">ðŸŽ¤</span><span class="message-time">${timestamp}</span></div><div class="message-content">${content}</div>`;
            break;
            
        case 'screenshot':
            messageContent = `<div class="message-header"><span class="message-icon">ðŸ“¸</span><span class="message-time">${timestamp}</span></div><div class="message-content">Screenshot captured</div>`;
            break;
            
        case 'ai-response':
            messageContent = `<div class="message-header"><span class="message-icon">ðŸ¤–</span><span class="message-time">${timestamp}</span></div><div class="message-content ai-response">${formatResponse(content)}</div>`;
            break;
            
        case 'system':
            messageContent = `<div class="message-header"><span class="message-icon">â„¹ï¸</span><span class="message-time">${timestamp}</span></div><div class="message-content system-message">${content}</div>`;
            break;
    }
    
    messageDiv.innerHTML = messageContent;
    chatMessagesElement.appendChild(messageDiv);
    
    chatMessagesElement.scrollTop = chatMessagesElement.scrollHeight;
    
    chatMessagesArray.push({
        type,
        content,
        timestamp: new Date()
    });
}

// Start timer
function startTimer() {
    const timerElement = document.querySelector('.timer');
    if (!timerElement) return;
    
    timerInterval = setInterval(() => {
        const elapsed = Math.floor((Date.now() - startTime) / 1000);
        const minutes = Math.floor(elapsed / 60).toString().padStart(2, '0');
        const seconds = (elapsed % 60).toString().padStart(2, '0');
        timerElement.textContent = `${minutes}:${seconds}`;
    }, 1000);
}

// Setup event listeners
function setupEventListeners() {
    if (screenshotBtn) screenshotBtn.addEventListener('click', takeStealthScreenshot);
    if (analyzeBtn) analyzeBtn.addEventListener('click', analyzeScreenshots);
    if (clearBtn) clearBtn.addEventListener('click', clearStealthData);
    if (hideBtn) hideBtn.addEventListener('click', emergencyHide);
    if (copyBtn) copyBtn.addEventListener('click', copyToClipboard);
    if (closeResultsBtn) closeResultsBtn.addEventListener('click', hideResults);
    if (voiceToggle) voiceToggle.addEventListener('click', toggleVoiceRecognition);

    document.addEventListener('keydown', (e) => {
        if (e.ctrlKey && e.altKey && e.shiftKey) {
            switch (e.key.toLowerCase()) {
                case 'h':
                    e.preventDefault();
                    if (window.electronAPI) window.electronAPI.toggleStealth();
                    break;
                case 's':
                    e.preventDefault();
                    takeStealthScreenshot();
                    break;
                case 'a':
                    e.preventDefault();
                    analyzeScreenshots();
                    break;
                case 'x':
                    e.preventDefault();
                    emergencyHide();
                    break;
                case 'v':
                    e.preventDefault();
                    toggleVoiceRecognition();
                    break;
            }
        }
    });

    document.addEventListener('contextmenu', e => e.preventDefault());
    document.addEventListener('selectstart', e => e.preventDefault());
    document.addEventListener('dragstart', e => e.preventDefault());
}

// Setup IPC listeners
function setupIpcListeners() {
    if (!window.electronAPI) {
        console.error('electronAPI not available');
        return;
    }

    window.electronAPI.onScreenshotTakenStealth((count) => {
        screenshotsCount = count;
        updateUI();
        addChatMessage('screenshot', 'Screenshot captured');
        showFeedback('Screenshot captured', 'success');
    });

    window.electronAPI.onAnalysisStart(() => {
        setAnalyzing(true);
        showLoadingOverlay();
        addChatMessage('system', 'Analyzing screenshots and context...');
    });

    window.electronAPI.onAnalysisResult((data) => {
        setAnalyzing(false);
        hideLoadingOverlay();
        
        if (data.error) {
            addChatMessage('system', `Error: ${data.error}`);
            showFeedback(data.error, 'error');
        } else {
            addChatMessage('ai-response', data.text);
            showFeedback('Analysis complete', 'success');
        }
    });

    window.electronAPI.onSetStealthMode((enabled) => {
        setStealthMode(enabled);
    });

    window.electronAPI.onEmergencyClear(() => {
        hideResults();
        clearStealthData();
        emergencyHide();
    });

    window.electronAPI.onError((message) => {
        addChatMessage('system', `Error: ${message}`);
        showFeedback(message, 'error');
    });
}

// Take screenshot
async function takeStealthScreenshot() {
    if (!window.electronAPI) {
        showFeedback('electronAPI not available', 'error');
        return;
    }

    try {
        screenshotBtn.disabled = true;
        await window.electronAPI.takeStealthScreenshot();
    } catch (error) {
        console.error('Screenshot error:', error);
        showFeedback('Screenshot failed', 'error');
        addChatMessage('system', 'Screenshot failed');
    } finally {
        screenshotBtn.disabled = false;
    }
}

// Analyze screenshots
async function analyzeScreenshots() {
    if (screenshotsCount === 0) {
        showFeedback('No screenshots to analyze', 'error');
        return;
    }

    if (!window.electronAPI) {
        showFeedback('electronAPI not available', 'error');
        return;
    }

    try {
        const recentMessages = chatMessagesArray.slice(-10).filter(msg => 
            msg.type === 'voice' || msg.type === 'ai-response'
        );
        
        const context = recentMessages.map(msg => 
            `${msg.type === 'voice' ? 'User' : 'AI'}: ${msg.content}`
        ).join('\n');
        
        await window.electronAPI.analyzeStealthWithContext(context);
    } catch (error) {
        console.error('Analysis error:', error);
        showFeedback('Analysis failed', 'error');
        addChatMessage('system', 'Analysis failed');
    }
}

// Clear data
async function clearStealthData() {
    if (!window.electronAPI) return;

    try {
        clearBtn.disabled = true;
        const result = await window.electronAPI.clearStealth();
        
        if (result.success) {
            screenshotsCount = 0;
            chatMessagesElement.innerHTML = '';
            chatMessagesArray.length = 0;
            hideResults();
            updateUI();
            addChatMessage('system', 'Data cleared');
            showFeedback('Data cleared', 'info');
        }
    } catch (error) {
        console.error('Clear error:', error);
        showFeedback('Clear failed', 'error');
    } finally {
        clearBtn.disabled = false;
    }
}

// Emergency hide
function emergencyHide() {
    if (emergencyOverlay) {
        emergencyOverlay.classList.remove('hidden');
    }
    
    if (isRecording) {
        stopAutoRecording();
        voiceToggle.classList.remove('active');
    }
    
    if (resultText) {
        resultText.textContent = '';
    }
    
    setTimeout(() => {
        if (emergencyOverlay) {
            emergencyOverlay.classList.add('hidden');
        }
    }, 2000);
}

// Utility functions
function hideResults() {
    if (resultsPanel) {
        resultsPanel.classList.add('hidden');
    }
    if (resultText) {
        resultText.innerHTML = '';
    }
}

function formatResponse(text) {
    let formatted = text;
    formatted = formatted.replace(/```(\w+)?\n([\s\S]*?)```/g, '<div class="code-block"><code>$2</code></div>');
    formatted = formatted.replace(/`([^`]+)`/g, '<code>$1</code>');
    formatted = formatted.replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>');
    formatted = formatted.replace(/â€¢ (.*?)(?=\n|$)/g, '<div>â€¢ $1</div>');
    formatted = formatted.replace(/\n/g, '<br>');
    return formatted;
}

async function copyToClipboard() {
    if (!chatMessagesElement || chatMessagesElement.children.length === 0) {
        showFeedback('No content to copy', 'error');
        return;
    }

    try {
        const lastAiMessage = Array.from(chatMessagesElement.children)
            .reverse()
            .find(msg => msg.classList.contains('ai-response'));
        
        if (lastAiMessage) {
            const text = lastAiMessage.querySelector('.message-content').textContent;
            await navigator.clipboard.writeText(text);
            showFeedback('Copied to clipboard', 'success');
        } else {
            showFeedback('No AI response to copy', 'error');
        }
    } catch (error) {
        console.error('Copy error:', error);
        showFeedback('Copy failed', 'error');
    }
}

function updateUI() {
    if (screenshotCount) {
        screenshotCount.textContent = screenshotsCount;
    }
    
    if (analyzeBtn) {
        analyzeBtn.disabled = screenshotsCount === 0 || isAnalyzing;
    }
    if (clearBtn) {
        clearBtn.disabled = screenshotsCount === 0 && chatMessagesElement.children.length === 0;
    }
    
    updateVoiceUI();
}

function setAnalyzing(analyzing) {
    isAnalyzing = analyzing;
    updateUI();
}

function setStealthMode(enabled) {
    if (stealthHideTimeout) {
        clearTimeout(stealthHideTimeout);
        stealthHideTimeout = null;
    }

    stealthModeActive = enabled;
    
    if (enabled) {
        document.body.classList.add('stealth-mode');
        if (isRecording) {
            stopAutoRecording();
            voiceToggle.classList.remove('active');
        }
    } else {
        document.body.classList.remove('stealth-mode');
    }
}

function showLoadingOverlay() {
    if (loadingOverlay) {
        loadingOverlay.classList.remove('hidden');
    }
}

function hideLoadingOverlay() {
    if (loadingOverlay) {
        loadingOverlay.classList.add('hidden');
    }
}

function showFeedback(message, type) {
    type = type || 'info';
    
    if (!statusText) {
        console.log('Status feedback:', message, type);
        return;
    }
    
    statusText.style.display = 'block';
    statusText.style.opacity = '1';
    statusText.textContent = message;
    statusText.className = `status-text status-${type} show`;
    
    setTimeout(() => {
        statusText.style.opacity = '0';
        setTimeout(() => {
            statusText.style.display = 'none';
            statusText.className = 'status-text';
        }, 300);
    }, 3000);
}

// Initialize when DOM is ready
document.addEventListener('DOMContentLoaded', async () => {
    console.log('DOM loaded, initializing...');
    await init();
    
    stealthModeActive = false;
    
    setTimeout(() => {
        addChatMessage('system', 'AI Meeting Assistant with Whisper speech recognition ready. Click the microphone to start recording.');
    }, 1000);
});

// Cleanup
window.addEventListener('beforeunload', () => {
    if (timerInterval) {
        clearInterval(timerInterval);
    }
    if (stealthHideTimeout) {
        clearTimeout(stealthHideTimeout);
    }
    if (isRecording) {
        stopAutoRecording();
    }
    if (audioStream) {
        audioStream.getTracks().forEach(track => track.stop());
    }
    if (worker) {
        worker.terminate();
    }
    if (audioContext && audioContext.state !== 'closed') {
        audioContext.close();
    }
});

================
File: src/styles.css
================
/* COMPACT GLASS OVERLAY - CLUELY STYLE WITH CHAT INTERFACE */

* {
  margin: 0;
  padding: 0;
  box-sizing: border-box;
  -webkit-user-select: none;
  user-select: none;
}

body {
  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
  background: transparent !important;
  color: #000;
  overflow: hidden;
  cursor: default;
  font-size: 11px;
  line-height: 1.3;
  width: 100vw;
  height: 100vh;
  margin: 0;
  padding: 0;
}

/* COMPACT GLASS CONTAINER - LIKE CLUELY */
.glass-container {
  width: 100vw;
  height: 100vh;
  background: rgba(255, 255, 255, 0.15);
  backdrop-filter: blur(40px) saturate(1.8);
  -webkit-backdrop-filter: blur(40px) saturate(1.8);
  border: 1px solid rgba(255, 255, 255, 0.2);
  border-radius: 12px;
  padding: 8px;
  position: relative;
  
  /* Cluely-style glass effect */
  box-shadow: 
    0 8px 32px rgba(0, 0, 0, 0.1),
    inset 0 1px 0 rgba(255, 255, 255, 0.2);
  
  -webkit-app-region: no-drag;
  opacity: 1;
  transition: all 0.3s ease;
  display: flex;
  flex-direction: column;
  gap: 8px;
}

/* COMPACT LAYOUT - HORIZONTAL */
.main-interface {
  display: flex;
  flex-direction: row;
  align-items: center;
  gap: 8px;
  flex-shrink: 0;
  padding: 0;
}

/* COMPACT STATUS */
.status-section {
  display: flex;
  align-items: center;
  gap: 6px;
  padding: 4px 8px;
  background: rgba(255, 255, 255, 0.2);
  border-radius: 8px;
  border: 1px solid rgba(255, 255, 255, 0.3);
  backdrop-filter: blur(20px);
}

.status-dot {
  width: 6px;
  height: 6px;
  background: #00ff88;
  border-radius: 50%;
  animation: pulse 2s infinite;
}

.timer {
  font-size: 11px;
  font-weight: 600;
  color: rgba(0, 0, 0, 0.8);
  font-family: 'SF Mono', Monaco, Consolas, monospace;
  min-width: 35px;
}

.screenshot-count {
  background: rgba(59, 130, 246, 0.8);
  color: white;
  padding: 2px 6px;
  border-radius: 8px;
  font-size: 10px;
  font-weight: 600;
  min-width: 16px;
  text-align: center;
  backdrop-filter: blur(10px);
}

@keyframes pulse {
  0%, 100% { opacity: 1; transform: scale(1); }
  50% { opacity: 0.5; transform: scale(1.2); }
}

/* COMPACT CONTENT */
.content-section {
  display: flex;
  align-items: center;
  gap: 8px;
  flex: 1;
}

.ai-status {
  display: none; /* Hidden in compact mode */
}

/* HIDDEN STATUS TEXT FOR FEEDBACK */
.status-text {
  position: absolute;
  top: -30px;
  left: 50%;
  transform: translateX(-50%);
  background: rgba(0, 0, 0, 0.8);
  color: white;
  padding: 4px 8px;
  border-radius: 6px;
  font-size: 10px;
  font-weight: 600;
  white-space: nowrap;
  backdrop-filter: blur(10px);
  opacity: 0;
  transition: opacity 0.3s ease;
  pointer-events: none;
  z-index: 1000;
}

.status-text.show {
  opacity: 1;
}

/* COMPACT ACTION BUTTONS */
.action-buttons {
  display: flex;
  gap: 4px;
  align-items: center;
}

.action-btn {
  background: rgba(255, 255, 255, 0.2);
  border: 1px solid rgba(255, 255, 255, 0.3);
  color: rgba(0, 0, 0, 0.8);
  padding: 6px 8px;
  border-radius: 6px;
  font-size: 10px;
  font-weight: 600;
  cursor: pointer;
  transition: all 0.2s cubic-bezier(0.4, 0, 0.2, 1);
  backdrop-filter: blur(20px);
  display: flex;
  align-items: center;
  justify-content: center;
  gap: 4px;
  min-width: 24px;
  height: 24px;
  white-space: nowrap;
}

.action-btn:hover {
  background: rgba(255, 255, 255, 0.3);
  transform: translateY(-1px);
  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.15);
}

.action-btn.voice-btn {
  background: rgba(34, 197, 94, 0.2);
  border-color: rgba(34, 197, 94, 0.4);
}

.action-btn.voice-btn.active {
  background: rgba(34, 197, 94, 0.8);
  color: white;
  animation: voicePulse 1.5s infinite;
}

.action-btn.voice-btn.listening {
  background: rgba(239, 68, 68, 0.8);
  color: white;
  animation: recording 1s infinite;
}

.action-btn.analyze-btn {
  background: rgba(59, 130, 246, 0.8);
  border-color: rgba(59, 130, 246, 0.9);
  color: white;
  font-weight: 700;
  padding: 6px 10px;
}

.action-btn.analyze-btn:hover {
  background: rgba(37, 99, 235, 0.9);
  box-shadow: 0 4px 16px rgba(59, 130, 246, 0.4);
}

.action-btn:disabled {
  opacity: 0.4;
  cursor: not-allowed;
  transform: none;
}

@keyframes voicePulse {
  0%, 100% { transform: scale(1); }
  50% { transform: scale(1.05); }
}

@keyframes recording {
  0%, 100% { opacity: 1; }
  50% { opacity: 0.6; }
}

/* CHAT INTERFACE */
.chat-container {
  flex: 1;
  display: flex;
  flex-direction: column;
  background: rgba(255, 255, 255, 0.1);
  backdrop-filter: blur(20px);
  border: 1px solid rgba(255, 255, 255, 0.2);
  border-radius: 8px;
  overflow: hidden;
  min-height: 200px;
  max-height: 400px;
}

.chat-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 6px 10px;
  background: rgba(255, 255, 255, 0.2);
  border-bottom: 1px solid rgba(255, 255, 255, 0.2);
  backdrop-filter: blur(20px);
}

.chat-title {
  font-size: 11px;
  font-weight: 700;
  color: rgba(0, 0, 0, 0.8);
  display: flex;
  align-items: center;
  gap: 4px;
}

.chat-controls {
  display: flex;
  gap: 4px;
}

.copy-btn {
  background: rgba(255, 255, 255, 0.2);
  border: 1px solid rgba(255, 255, 255, 0.3);
  color: rgba(0, 0, 0, 0.8);
  padding: 4px;
  border-radius: 4px;
  cursor: pointer;
  transition: all 0.2s ease;
  width: 20px;
  height: 20px;
  display: flex;
  align-items: center;
  justify-content: center;
  backdrop-filter: blur(10px);
}

.copy-btn:hover {
  background: rgba(34, 197, 94, 0.3);
  transform: scale(1.05);
}

.chat-messages {
  flex: 1;
  overflow-y: auto;
  padding: 8px;
  display: flex;
  flex-direction: column;
  gap: 6px;
}

.chat-message {
  display: flex;
  flex-direction: column;
  gap: 2px;
  padding: 6px 8px;
  border-radius: 8px;
  backdrop-filter: blur(10px);
  transition: all 0.2s ease;
}

.chat-message.voice {
  background: rgba(34, 197, 94, 0.1);
  border-left: 3px solid rgba(34, 197, 94, 0.6);
}

.chat-message.screenshot {
  background: rgba(59, 130, 246, 0.1);
  border-left: 3px solid rgba(59, 130, 246, 0.6);
}

.chat-message.ai-response {
  background: rgba(147, 51, 234, 0.1);
  border-left: 3px solid rgba(147, 51, 234, 0.6);
}

.chat-message.system {
  background: rgba(156, 163, 175, 0.1);
  border-left: 3px solid rgba(156, 163, 175, 0.6);
}

.chat-message.interim {
  background: rgba(255, 193, 7, 0.1);
  border-left: 3px solid rgba(255, 193, 7, 0.6);
  opacity: 0.8;
}

.message-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  gap: 6px;
  margin-bottom: 2px;
}

.message-icon {
  font-size: 12px;
  display: flex;
  align-items: center;
}

.message-time {
  font-size: 9px;
  color: rgba(0, 0, 0, 0.6);
  font-family: 'SF Mono', Monaco, Consolas, monospace;
}

.message-content {
  font-size: 10px;
  line-height: 1.4;
  color: rgba(0, 0, 0, 0.8);
  -webkit-user-select: text;
  user-select: text;
  word-wrap: break-word;
}

.message-content.interim-content {
  opacity: 0.7;
  font-style: italic;
}

.message-content.system-message {
  color: rgba(0, 0, 0, 0.6);
  font-style: italic;
}

.message-content.ai-response {
  color: rgba(0, 0, 0, 0.9);
  font-weight: 500;
}

/* HIDE SETTINGS IN COMPACT MODE */
.settings-section {
  display: none;
}

/* COMPACT RESULTS PANEL - LEGACY */
.results-panel {
  position: absolute;
  top: 130px;
  left: 0;
  right: 0;
  width: 400px;
  max-height: 300px;
  background: rgba(255, 255, 255, 0.15);
  backdrop-filter: blur(40px) saturate(1.8);
  border: 1px solid rgba(255, 255, 255, 0.2);
  border-radius: 12px;
  box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
  display: flex;
  flex-direction: column;
  z-index: 100;
}

.results-panel.hidden {
  display: none;
}

.results-header {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 8px 12px;
  border-bottom: 1px solid rgba(255, 255, 255, 0.2);
  backdrop-filter: blur(20px);
}

.results-title {
  display: flex;
  align-items: center;
  gap: 6px;
  font-size: 11px;
  font-weight: 700;
  color: rgba(0, 0, 0, 0.8);
}

.ai-response-icon {
  font-size: 12px;
}

.results-actions {
  display: flex;
  gap: 4px;
}

.close-btn {
  background: rgba(255, 255, 255, 0.2);
  border: 1px solid rgba(255, 255, 255, 0.3);
  color: rgba(0, 0, 0, 0.8);
  padding: 4px;
  border-radius: 4px;
  cursor: pointer;
  transition: all 0.2s ease;
  width: 20px;
  height: 20px;
  display: flex;
  align-items: center;
  justify-content: center;
  backdrop-filter: blur(10px);
}

.close-btn:hover {
  background: rgba(255, 0, 0, 0.3);
}

.results-content {
  flex: 1;
  padding: 12px;
  overflow-y: auto;
  max-height: 250px;
}

.result-text {
  font-size: 11px;
  line-height: 1.4;
  color: rgba(0, 0, 0, 0.8);
  white-space: pre-wrap;
  word-wrap: break-word;
}

/* CODE FORMATTING */
.result-text code, .message-content code {
  background: rgba(59, 130, 246, 0.2);
  color: rgba(30, 64, 175, 0.9);
  padding: 1px 3px;
  border-radius: 3px;
  font-family: 'SF Mono', Monaco, Consolas, monospace;
  font-size: 9px;
}

.result-text .code-block, .message-content .code-block {
  background: rgba(0, 0, 0, 0.8);
  color: #00ff88;
  border: 1px solid rgba(255, 255, 255, 0.1);
  border-radius: 6px;
  padding: 6px;
  margin: 6px 0;
  font-family: 'SF Mono', Monaco, Consolas, monospace;
  font-size: 9px;
  line-height: 1.3;
  overflow-x: auto;
  backdrop-filter: blur(10px);
}

.result-text .logic-section, .message-content .logic-section {
  background: rgba(59, 130, 246, 0.15);
  border-left: 3px solid rgba(59, 130, 246, 0.6);
  padding: 6px;
  margin: 6px 0;
  border-radius: 0 6px 6px 0;
  backdrop-filter: blur(10px);
}

.result-text .key-points, .message-content .key-points {
  background: rgba(255, 215, 0, 0.15);
  border-left: 3px solid rgba(255, 215, 0, 0.6);
  padding: 6px;
  margin: 6px 0;
  border-radius: 0 6px 6px 0;
  backdrop-filter: blur(10px);
}

.result-text strong, .message-content strong {
  color: rgba(0, 0, 0, 0.9);
  font-weight: 700;
}

/* COMPACT LOADING OVERLAY */
.loading-overlay {
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: rgba(255, 255, 255, 0.2);
  backdrop-filter: blur(20px);
  display: flex;
  flex-direction: row;
  align-items: center;
  justify-content: center;
  border-radius: 12px;
  z-index: 200;
  gap: 8px;
}

.loading-overlay.hidden {
  display: none;
}

.loading-content {
  display: flex;
  align-items: center;
  gap: 8px;
}

.analyzing-icon {
  font-size: 16px;
  animation: bounce 1s infinite;
}

.loading-text {
  font-size: 11px;
  color: rgba(0, 0, 0, 0.8);
  font-weight: 600;
}

@keyframes bounce {
  0%, 100% { transform: scale(1); }
  50% { transform: scale(1.1); }
}

/* EMERGENCY OVERLAY - PRESERVES STEALTH */
.emergency-overlay {
  position: absolute;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background: rgba(0, 0, 0, 0.95);
  display: flex;
  align-items: center;
  justify-content: center;
  border-radius: 12px;
  backdrop-filter: blur(40px);
  z-index: 300;
}

.emergency-overlay.hidden {
  display: none;
}

.emergency-message {
  color: #ff4444;
  font-weight: 700;
  font-size: 12px;
  animation: blink 1s infinite;
}

@keyframes blink {
  0%, 50% { opacity: 1; }
  51%, 100% { opacity: 0.3; }
}

/* STEALTH MODE - DARKER GLASS */
.stealth-mode .glass-container {
  background: rgba(20, 25, 35, 0.3);
  border: 1px solid rgba(255, 255, 255, 0.1);
  opacity: 0.7;
  transition: all 0.5s ease;
}

.stealth-mode * {
  opacity: 0.8 !important;
  transition: opacity 0.5s ease;
}

.stealth-mode:hover * {
  opacity: 1 !important;
  transition: opacity 0.2s ease;
}

/* STATUS STYLES */
.status-success { color: #00ff88 !important; }
.status-error { color: #ff6b6b !important; }
.status-info { color: #74b9ff !important; }

/* SCROLLBAR STYLING */
.chat-messages::-webkit-scrollbar,
.results-content::-webkit-scrollbar {
  width: 4px;
}

.chat-messages::-webkit-scrollbar-track,
.results-content::-webkit-scrollbar-track {
  background: rgba(255, 255, 255, 0.1);
  border-radius: 2px;
}

.chat-messages::-webkit-scrollbar-thumb,
.results-content::-webkit-scrollbar-thumb {
  background: rgba(255, 255, 255, 0.3);
  border-radius: 2px;
}

.chat-messages::-webkit-scrollbar-thumb:hover,
.results-content::-webkit-scrollbar-thumb:hover {
  background: rgba(255, 255, 255, 0.4);
}

/* ENTRANCE ANIMATION */
@keyframes fadeInGlass {
  from { 
    opacity: 0; 
    transform: scale(0.95) translateY(-10px);
    backdrop-filter: blur(0px);
  }
  to { 
    opacity: 1; 
    transform: scale(1) translateY(0);
    backdrop-filter: blur(40px);
  }
}

.glass-container {
  animation: fadeInGlass 0.4s cubic-bezier(0.4, 0, 0.2, 1);
}

/* CHAT MESSAGE ANIMATIONS */
.chat-message {
  animation: slideInMessage 0.3s ease-out;
}

@keyframes slideInMessage {
  from {
    opacity: 0;
    transform: translateY(10px);
  }
  to {
    opacity: 1;
    transform: translateY(0);
  }
}

/* VOICE BUTTON SPECIFIC STYLES */
.voice-btn svg {
  transition: all 0.2s ease;
}

.voice-btn.listening svg {
  color: white;
  filter: drop-shadow(0 0 4px rgba(239, 68, 68, 0.8));
}

.voice-btn.active svg {
  color: white;
  filter: drop-shadow(0 0 4px rgba(34, 197, 94, 0.8));
}

/* RESPONSIVE ADJUSTMENTS */
@media (max-height: 600px) {
  .chat-container {
    max-height: 300px;
  }
}

@media (max-height: 400px) {
  .chat-container {
    max-height: 200px;
  }
}

================
File: src/whisper-worker.js
================
/* eslint-disable camelcase */
// REPLACE your entire whisper-worker.js with this exact working version
import { pipeline, env } from "@xenova/transformers";

// Disable local models
env.allowLocalModels = false;

// Define model factories
// Ensures only one model is created of each type
class PipelineFactory {
    static task = null;
    static model = null;
    static quantized = null;
    static instance = null;

    constructor(tokenizer, model, quantized) {
        this.tokenizer = tokenizer;
        this.model = model;
        this.quantized = quantized;
    }

    static async getInstance(progress_callback = null) {
        if (this.instance === null) {
            this.instance = pipeline(this.task, this.model, {
                quantized: this.quantized,
                progress_callback,
                // For medium models, we need to load the `no_attentions` revision to avoid running out of memory
                revision: this.model.includes("/whisper-medium") ? "no_attentions" : "main"
            });
        }
        return this.instance;
    }
}

self.addEventListener("message", async (event) => {
    const message = event.data;

    // Do transcription work
    let transcript = await transcribe(
        message.audio,
        message.model,
        message.multilingual,
        message.quantized,
        message.subtask,
        message.language,
    );
    if (transcript === null) return;

    // Send the result back to the main thread
    self.postMessage({
        status: "complete",
        task: "automatic-speech-recognition",
        data: transcript,
    });
});

class AutomaticSpeechRecognitionPipelineFactory extends PipelineFactory {
    static task = "automatic-speech-recognition";
    static model = null;
    static quantized = null;
}

const transcribe = async (
    audio,
    model,
    multilingual,
    quantized,
    subtask,
    language,
) => {
    const isDistilWhisper = model.startsWith("distil-whisper/");

    let modelName = model;
    if (!isDistilWhisper && !multilingual) {
        modelName += ".en"
    }

    const p = AutomaticSpeechRecognitionPipelineFactory;
    if (p.model !== modelName || p.quantized !== quantized) {
        // Invalidate model if different
        p.model = modelName;
        p.quantized = quantized;

        if (p.instance !== null) {
            (await p.getInstance()).dispose();
            p.instance = null;
        }
    }

    // Load transcriber model
    let transcriber = await p.getInstance((data) => {
        self.postMessage(data);
    });

    const time_precision =
        transcriber.processor.feature_extractor.config.chunk_length /
        transcriber.model.config.max_source_positions;

    // Storage for chunks to be processed. Initialise with an empty chunk.
    let chunks_to_process = [
        {
            tokens: [],
            finalised: false,
        },
    ];

    function chunk_callback(chunk) {
        let last = chunks_to_process[chunks_to_process.length - 1];

        // Overwrite last chunk with new info
        Object.assign(last, chunk);
        last.finalised = true;

        // Create an empty chunk after, if it not the last chunk
        if (!chunk.is_last) {
            chunks_to_process.push({
                tokens: [],
                finalised: false,
            });
        }
    }

    // Inject custom callback function to handle merging of chunks
    function callback_function(item) {
        let last = chunks_to_process[chunks_to_process.length - 1];

        // Update tokens of last chunk
        last.tokens = [...item[0].output_token_ids];

        // Merge text chunks
        let data = transcriber.tokenizer._decode_asr(chunks_to_process, {
            time_precision: time_precision,
            return_timestamps: true,
            force_full_sequences: false,
        });

        self.postMessage({
            status: "update",
            task: "automatic-speech-recognition",
            data: data,
        });
    }

    // Actually run transcription
    let output = await transcriber(audio, {
        // Greedy
        top_k: 0,
        do_sample: false,

        // Sliding window
        chunk_length_s: isDistilWhisper ? 20 : 30,
        stride_length_s: isDistilWhisper ? 3 : 5,

        // Language and task
        language: language,
        task: subtask,

        // Return timestamps
        return_timestamps: true,
        force_full_sequences: false,

        // Callback functions
        callback_function: callback_function, // after each generation step
        chunk_callback: chunk_callback, // after each chunk is processed
    }).catch((error) => {
        self.postMessage({
            status: "error",
            task: "automatic-speech-recognition",
            data: error,
        });
        return null;
    });

    return output;
};
